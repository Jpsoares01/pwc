<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title></title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="/css/app.css">
</head>
<body style="background-color: #F5E7C6;">

    <div id="header-container"></div>

    <div class="mid-content-wrapper">
    <div id="aside-content"></div>

    <div id="body", class="body">
        <h1> Acessibilidade</h1>

        <div id="evolScreenRead", class="section-container">
            <div><h4>The Evolution of Screen Reading Technology</h4></div>
            <div class="section-content-container">
                <div class="paragraph-container">
                    <p>Screen reading began with Optical Character Recognition (OCR) between the 1950s and 70s, allowing machines to "read" printed text. In the 80s and 90s, iconic software like JAWS, NVDA, and VoiceOver emerged, guaranteeing autonomous voice navigation in systems like Windows and macOS for visually impaired users.</p> 
                    <p>The introduction of Deep Learning (AI) transcended textual limitations, turning screen reading into a complete, multimodal visual analysis. This enables modern applications (such as OrCam and Seeing AI) that can interpret images, describe visual contexts, identify objects, and read signs, representing a crucial advance toward a more inclusive world.</p>
                </div>
                <div class="image-container">
                    <img src="/media/index_img/screen_reader_ba.jpeg" >
                </div>
            </div>
        </div>

        <!---->
        <div id="digBarriers", class="section-container">
            <div><h4>Digital Barriers: The Accessibility Revolution by AI</h4></div>
            <div class="section-content-container">
                <div class="paragraph-container">
                    <p>Before AI and advanced screen-reading technologies, digital information was inaccessible to visually impaired people, mainly due to poorly structured web code. Websites and applications often failed to correctly label buttons and HTML elements, resulting in chaotic navigation for screen readers.</p>
                    <p>Furthermore, formats like printed documents and text embedded in images were completely "mute" to older software, requiring constant human assistance. Artificial Intelligence (AI) and advanced OCR overcame these flaws, acting as a "translation layer" that interprets broken code and automatically describes visual elements. This restored autonomy to users, compensating for human accessibility failures.</p>
                </div>
                <div class="image-container">
                    <img src="media/index_img/acessibility_revolution.png" >
                </div>
            </div>
        </div>
                <!-- -->
        <div id="ReleData", class="section-container">
            <div><h4>Relevant Data</h4></div>
            <div class="section-content-container">
                <div class="paragraph-container">
                    <p>Recent studies indicate that over 2.2 billion people worldwide live with some degree of visual impairment, while only a fraction of websites meet digital accessibility standards (WCAG). Integrating AI-based solutions in this field can significantly reduce this gap, improving usability and digital inclusion. Collecting and analyzing system performance data, such as recognition accuracy, response time, and user satisfaction levels, will be essential for the continuous improvement of the technology.</p>
                    <p>According to the findings by Li, Lan, and Zhou (2025), compression method, which converts verbose text contexts into compact visual tokens, offers a compelling solution for developing efficient, tiny AI models. Effectively, treating the entire screen's content as a visual compressed input allows the model to quickly reason over the page's structure and suggest improvements without the computational burden of processing the original, uncompressed text tokens, thereby fulfilling the goal of improving digital inclusion under strict efficiency constraints.</p>
                    <a href="https://aclanthology.org/2025.findings-emnlp.558/" target="_blank">
                        <img style="width: 200px; height: 266px;" src="media/index_img/studie.png" alt="">
                    </a>
                    </div>
            </div>
        </div>
        <!-- -->
        <div id="auraView", class="section-container">
            <div><h4>AuraView</h4></div>
            <div class="section-content-container">
                <div class="paragraph-container">
                    <p>The development of an AI-assisted screen reader represents a significant advancement in digital accessibility. This system is capable of automatically capturing screenshots and recognizing the displayed content (text, icons, buttons, or images), transforming this information into understandable descriptions for users with visual impairments. The initiative combines computer vision and natural language processing to make any digital interface more inclusive and intuitive.</p>
                    </div>
                <div class="image-container">
                    <img src="media/index_img/auraView.png" >
                </div>
            </div>
        </div>

        </div>
    <script src="js/app.js"></script>
    <script src="js/custom/_topbar.js"></script>
</body>
</html>